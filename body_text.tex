
\section{Summary}
\label{sec:org318c80c}
\begin{itemize}
\item Tracking survey progress, comparison of the actual progress both to a baseline and to what would have been expected under similar conditions (e.g. weather) is important for Rubin Observatory staff and management, the science collaborations, the astronomical community, funding agencies, and even the general public.
\item Survey progress and strategy diagnostics will need to be presented in multiple contexts, to a variety of audiences.
\item Data useful for evaluating survey progress and optomizing strategy is best visualized in a variety of ways. Some useful metrics are best viewed as scalars, or a simple time series. In other cases, complex plots or diagrams are required, and even then such plots may only be effectively interpreted in combination with other plots, diagrams, or descriptive text.
\item Some metrics and visualizations needed for evaluating survey progress and optomizing strategy is "close to the data," and require minimal processing before presentation to the user. In other cases, however, significant analysis is necessary (for example suites of scheduler simulations). This diversity complicates infrastructure requirements.
\item Generation of survey progress and strategy diagnostics requires both data generated "close to the instrument" (telemetry from the telescope) and data only available from other sources such as the output of processing by Data Management and the results of strategy simulations.
\item Rubin Observatory infrastructure currently under development (including opsim, MAF, SQuaRE/SQuaSH, LOVE, the Engineering and Facilities Database (EFD), and the Data Management butler) provides or will provide an array data sources and visualzation tools. It is not always clear, however, which tools are a good match for which strategy and progress related metrics, and work is required take advantage of this existing infrastructure.
\end{itemize}
\section{{\bfseries\sffamily TODO} Introduction}
\label{sec:org710aa57}
\section{{\bfseries\sffamily ACTIVE} Tools, reports and their audiences}
\label{sec:orgd6687d9}
\subsection{Introduction}
\label{sec:org8e7b9e7}
The Rubin Observatory project and staff performing LSST will require survey progress and status diagnostics, including a variety of metrics and plots.
Some of these will be needed by the staff themselves, providing the data needed to prevent and diagnose problems, identify potential imporvements, and evaluate suggestions for changes.
In addition, such plots and metrics will also be needed for reports to the astronomical community and funding agencies, and even may be useful in engaging the general public.

The infrastructure suitable for producing such plots and metrics depend of several factors, including the audience expected to make use of them and the frequency with which they need to be produced.
Full automation of the production of plots and metrics will be most important when they need to be produced frequently, on a nightly or monthly basis.
When their audience includes non-experts, either full automation or simple production on demand will save effort.
Plots that are used primarily for debugging or exploration of specific issues may not require the same level of automation or simplicty of interface, but tools for reproduction of previous example of such diagnostics can be important for avoiding duplication of effort.

These plots and metrics can be produced and presented in any of several ways:

\begin{description}
\item[{Interactive tools}] When developing and debugging the software, hardware, and human procedures that produce the survey, experts working on the project require flexible tools to obtain and explore the relevant data. Planning and prediction of the consequences of events and choices will often benefit creation of simulations. Examples of such sets of tools (e.g. \texttt{opsim} and \texttt{MAF}) have been developed as part of project construction, and will continue to perform an important role in operations. General tools designed for monitoring other aspects of the survey (e.g. the health of the instrument or the status of data processing) will also have important roles to play.
\item[{Information dashboards}] Some plots and metrics will routine production and monitoring, often by those who are not expert users of the interactive tools like \texttt{MAF}. Even for those who do have the expertise, automation of the production of routine plots and metrics will save significant effort. Infrastructure that generates needed plots and metrics and presents them in a simple way (e.g. an automatically update web page, or small collection of web pages) will therefore be important. This infrastructure will require many of the same software components used by the interactive tools, plus some automation and presetation elements.
\item[{Reports}] The project will need to produce reports covering survey status and progress, whether in the form of documents and presentations. Many of the plots and metrics displayed in an information dashboard will be important elements in these reports.
\end{description}

The intended audience and the frequency of reporting are both important feature to consider in determining how any given metric or plot is to be generated.
Possible audiences include the funding agencies, Rubin Observatory management, LSST science collaboration scientists, the Rubin Observatory observing scientists, the scheduler developers, astronomers working on other projects, other members of the astronomical community, and the general public. 
Plots and metrics may be generated on regular schedules (nightly, monthly, etc.), or as occasions demand.

The LSST system and data management requirements (\href{https://ls.st/lse-29}{LSE-29} and \href{https://ls.st/lse-61}{LSE-61}) and observatory systems specifications (\href{https://ls.st/lse-30}{LSE-30}) include requirements on several reports and reporting tools, and additional reports or tools may also be required.

\subsection{Published upcomming schedule}
\label{sec:orge43ee3f}
To support coordination between LSST observing and that of other projects, including schedling of simultaneous or nearly simultaneous exposures the same areas of sky, the requirments specify that Rubin Observatory publish the observing schedule in advance.


One requirement that specifies the advanced schedule is \href{https://ls.st/lse-61}{LST-61}/DMS-REQ-0353, "Publishing predicted visit schedule":
\begin{quote}
Specification: A service shall be provided to publish to the community the next visit location and the predicted visit schedule provided by the OCS. This service shall consist of both a web page for human inspection and a web API to allow automated tools to respond promptly.

Discussion: The next visit and advanced schedule do not need to be published using the same service or protocol.
\end{quote}
another is \href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0378, "Advanced Publishing of Scheduler Sequence":
\begin{quote}
The scheduling of the observing sequence lasting at least \texttt{schedSeqDuration} shall be published in advance of each observing visit.
\end{quote}

Schedule predictions will be useful for other purposes as well, even if they are only approximate.
Another user for which an advanced schedule will be important is the observing scientists on duty.
If observing scientists can review simulated schedules for an upcomming night, it will help them both identify some undesireable behaviour before it occurrs, and also help the identify when something is going wrong during observing itself.
Many factors influence the scheduling of exposures in detail, and it can be difficult for a human to distinguish desired from undesired behaviour.
If the observing scientists can review a handful of different simulations of the upcomming night, they can consult with scheduling experts to understand any apparantely anomolous behaviour.
Then, if the behaviour of the scheduler during the night is significanty different from any simulated scheduled nights produced the day before, the observatory scientists may know that the scheduling experts need to be consulted even if the scheduler behaviour is not obviously wrong.\footnote{Additionally, the simulations that support the schedule predictions can serve as an emergency back-up in case of a disasterous failure of the scheduler during the night: the observing scientist can take the simulated schedule that seems most suitable given current conditions, and run it as a script.}
\subsection{Night reports}
\label{sec:orgb633637}
Night reports (or nights summaries) are in important feature common to most astronomical facilities, and basic plots and metrics indicating survey progress are important elements for such reports in large surveys such as LSST.
Several Rubin Observatory requirements require the generation of night reports:
\begin{description}
\item[{LSE-30/OSS-REQ-0131}] Nightly Summary Products
\item[{LSE-30/OSS-REQ-0406}] Subsystem Nightly Reporting
\item[{LSE-61/DMS-REQ-0096}] Generate Data Quality Report Within Specified Time
\item[{LSE-61/DMS-REQ-0097}] Level 1 Data Quality Report Definition
\end{description}
These specifications require that the report summarize "per system performance and behavior," but do not specify what is to be reported in great detail. LSE-61/DMS-REQ-0097, "Level 1 Data Quality Report Definition" has the most specificity:
\begin{quote}
The DMS shall produce a Level 1 Data Quality Report that contains indicators of data quality that result from running the DMS pipelines, including at least: Photometric zero point vs. time for each utilized filter; Sky brightness vs. time for each utilized filter; seeing vs. time for each utilized filter; PSF parameters vs. time for each utilized filter; detection efficiency for point sources vs. mag for each utilized filter.
\end{quote}

Several diagnostics will be of particulary important for monitoring the scheduler.
Some of these concern the scheduler itself, including distributions of slew angles, airmasses of exposures, time spent changing filters, and features that drive selection based on maintaing cadences.
In addition, performance indicators from several other subsystems are of great interest for scheduling purposes.
For example, scheduling simulations depend critically on slew times, filter change times, and a sky brightness model.
If the performance of the relavant systems (considering the "sky" a system) differs significantly from the scheduler's expectations, then the scheduler configuration will need to be updated, and perhaps reoptimized. 

\subsection{Weekly and monthly progress reports}
\label{sec:org3b7fa8c}
Among other duties, the "LSST Operations Plan" (LPM-73) specifies (section 8.3.3) that staff in Chile has the following telescope scheduling related duties:
\begin{itemize}
\item Tracking survey progress relative to the science requirements.
\item Optimizing the scheduled observations.
\item Balancing the observing schedule between survey operations, diagnostic activities, and
\end{itemize}
calibration.
Furthermore, that the staff in Chile
\begin{quote}
Use the Operations Simulator toolbox to measure progress against the survey plan. A written monthly progress report will be provided to Headquarters, and weekly updates will be tracked in Chile. Adjustments to the short term observing plan (choice of filters, relative priority of science programs, etc.) will be made in Chile. 
\end{quote}

\subsection{Periodic performance reviews}
\label{sec:orgfcf3503}
The observatory staff and scheduling team will need to report progress and strategic concerns to management, funding agencies, and the community as a whole.
These review may take the form of presentations (for example at the yearly Project and Community Workshop (PCW)) or written reports.
Current requirements as written acknoweldge the need for such reports, but do not significantly constrain their contents. 
\href{https://ls.st/lse-29}{LSE-29}/LSR-REQ-0065, "Survey performance reviews," states:
\begin{quote}
The Observatory shall have the ability to provide periodic status
reports on the progress of the survey to allow both operations staff
and the community to assess the survey progress.
\end{quote}
\href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0314, "Subsystem Performance Reporting", emphasizes the importance of comparison with baselines:
\begin{quote}
The LSST Observatory over the course of the 10-year survey shall monitor its performance with respect to its established baseline and report variances exceeding established thresholds.
\end{quote}
\href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0033, "Survey Planning and performance monitoring", calls out the need for reporting to the community at large:
\begin{quote}
The LSST shall provide the tools and administrative processes
necessary to monitor the progress of the ongoing survey, provide
reports on the progress of the survey, respond to feedback from the
science community, and evaluate the impact of changing science
priorities over the 10 year survey lifetime.

Discussion: It is expected that the performance of this task will
require the use of detailed survey simulations in order to evaluate
scheduling alternatives and optimize the future performance of the
survey.
\end{quote}

The reasonable contents of the reports will depend both on the venue and the state of the survey itself, and the reports themselves will require significant explanatory text and analysis: these reports are not a reasonable candidate for full automation.
However, many of the metrics and plots that will be contained in such reports are, and furthermore will usually be those already generated for Night reports (section \ref{sec:orgb633637}), weekly and monthly progress reports (section \ref{sec:org3b7fa8c}), or generated by dashboards or using interactive debugging tools.

\subsection{{\bfseries\sffamily ACTIVE} Tools for performance evaluation and analysis}
\label{sec:org6bee324}
In addition to reports, tools for monitoring performance ("dashboards") and interactive analysis are needed.
Some of the views of the data required will consist of simple values directly connected to data generated by the instrumentation of the observatory itself ("telemetry").
Such requirments include \href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0067, "Performance \& Trend Analysis Toolkit":
\begin{quote}
The LSST system shall provide a common tool kit for conducting performance analysis, including trending, on the telemetry captured in the Engineering \& Facility Database.
\end{quote}
and  \href{https://ls.st/lse-29}{LSE-29}/LSR-REQ-0071 "Scientific Oversight During Data Collection":
\begin{quote}
Requirement: The LSST Observatory shall be developed to allow an
observing scientist to have oversight of the Data Collection
process. This interaction shall be enabled either locally on the
summit or at remote locations. The data provided shall include all
observing condition data, telemetry data to assess telescope
conditions, and science data quality metrics for evaluation of the
data collection process.

Discussion: The objective of this requirement is to enable the
observing scientist to be involved directly in the observing
process. Under normal circumstances the observing scientist will not
intervene in the autonomous operations (LSR-REQ-0072), but should be
allowed to override if anomalous behavior occurs.
\end{quote}

In other cases, the it is clear that tools that are limited to simple examinition of telemetry will not be sufficient.
For example, \href{https://ls.st/lse-29}{LSE-29}/LSR-REQ-0066, "Survey performance evaluation":
\begin{quote}
The Project shall create the necessary survey performance evaluation
tools to predict the final results of the ten year survey based on the
actual survey completed to date, assess the impacts of survey strategy
changes resulting from changes in scientific priorities, and support
the planning of the survey on a variety of time scales, from nightly
through the entire 10 year duration.
\end{quote}
and \href{https://ls.st/lse-29}{LSE-29}/LSR-REQ-0070, "Science Priorities and Survey Monitoring":
\begin{quote}
The LSST project shall monitor the scientific and technical progress
of the survey, communicate with the scientific user community and
establish survey priorities, and adjust the survey design as needed to
accomplish its goals given these priorities and achieved performance.
\end{quote}
recognize the need for tools that support generation and analysis of higher level diagnostics.

Numerous additional requirements recognize needs for monitoring and reporting, and most of them are of significant relevance to survey progress and strategy. These include:
\begin{itemize}
\item \href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0056 System Monitoring \& Diagnostics
\item \href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0063 System Monitoring \& Diagnostics Subsystem Metadata for Science Analysis
\item \href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0068 Summit Environment Monitoring
\item \href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0072 Weather and Meteorological Monitoring
\item \href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0078 Maintenance Reporting
\item \href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0079 Maintenance Tracking and Analysis
\end{itemize}

TODO discuss separation of telemetry and other metrics that need tools for exploration, and correspondance to LOVE and SQuaRE/SQuaSH, and implications for survey progress monitoring tool development.

\subsection{Interfaces for education and public outreach}
\label{sec:orge91762f}
While many survey progress metrics and visualizations are only likely to be of interest to experts, several will be intuitive, and may be good candidates for engaging the general public, as per \href{https://ls.st/lse-29}{LSE-29}/LSR-REQ-0113, "EPO Products, Tools, and Interfaces"
\begin{quote}
LSST EPO shall provide access to LSST data through tools, interfaces,
and learning experiences that are designedto engage communities with
different levels of knowledge, experience and skills.
\end{quote}
Good candidates for presentation to the public are movies of numbers of exposures generated, and plots numbers of galaxies (or other objects) detected as a function of time.
\section{Reports elements}
\label{sec:orgbb2408d}
\begin{itemize}
\item {\bfseries\sffamily TODO} baseline numbers of exposures vs. time
\label{sec:org1ab4bcf}
\begin{itemize}
\item By band
\label{sec:org940f7a2}
\item By RA bin
\label{sec:org147c5bd}
\end{itemize}
\item {\bfseries\sffamily TODO} teff vs time
\label{sec:org6d0693c}
\begin{itemize}
\item By band
\label{sec:org7e4cc06}
\item By RA bin
\label{sec:org74504d3}
\end{itemize}
\item {\bfseries\sffamily TODO} Aarea at depths vs. time
\label{sec:org25ed627}
\item {\bfseries\sffamily TODO} Area times time at cadence
\label{sec:org2eafea4}
\item {\bfseries\sffamily TODO} Total cadence maps
\label{sec:orgf9cc8c9}
Maps of area on which we have achieved a given cadence for a given time.
\item {\bfseries\sffamily TODO} Active cadence maps
\label{sec:org5402458}
\item {\bfseries\sffamily TODO} Current values of science metrics vs. time
\label{sec:org1d919b5}
\item {\bfseries\sffamily TODO} Extrapolated values of science metrics vs. time
\label{sec:orgb39e94f}
\item {\bfseries\sffamily TODO} Survey movies
\label{sec:org94e487d}
\begin{itemize}
\item Nightly
\label{sec:orgc855c47}
\item Long-term
\label{sec:orge8a98ba}
\end{itemize}
\item {\bfseries\sffamily TODO} Current area meeting cadence criteria
\label{sec:org2bae485}
\item {\bfseries\sffamily TODO} Numbers of objects detected by time, location, magnitude
\label{sec:org86bb3b0}
\item {\bfseries\sffamily TODO} Scheduler feature maps
\label{sec:orgec9c716}
\item {\bfseries\sffamily TODO} Downtime (by whether expected and by cause)
\label{sec:orgf1ad1d9}
\begin{itemize}
\item weather
\label{sec:orgc0677ad}
\item technical problems
\label{sec:orga2190e9}
\end{itemize}
\item {\bfseries\sffamily TODO} Time spent on different programs (FWD, DDF, ToO, etc.)
\label{sec:orgbb05c3c}
\item {\bfseries\sffamily TODO} Overhead from instrument performance
\label{sec:orgab8bbce}
\begin{itemize}
\item slew time
\label{sec:org5b3c420}
\item filter change time
\label{sec:orgefe47a6}
\item readout time
\label{sec:org0845348}
\item total efficiency (exposure time/night time)
\label{sec:org649c8e7}
\end{itemize}
\item {\bfseries\sffamily TODO} Slew angle distributions
\label{sec:org51eddb7}
\item {\bfseries\sffamily TODO} Airmass distribution vs. time
\label{sec:orga070e00}
\item {\bfseries\sffamily TODO} Airmass distribution vs. location on sky
\label{sec:org600e96c}
\item {\bfseries\sffamily TODO} Delivered seeing distribution
\label{sec:org5cd06d5}
\item {\bfseries\sffamily TODO} Seeing achieved vs expected
\label{sec:org0cb07fa}
\item {\bfseries\sffamily TODO} Sky brightness achieved vs expected
\label{sec:orgc76e539}
\item {\bfseries\sffamily TODO} Weather (clouds) achieved vs expected
\label{sec:org637106c}
\item {\bfseries\sffamily TODO} DDF cadence evaluation plots
\label{sec:org1577521}
\item {\bfseries\sffamily TODO} WFD cadence evaluation maps
\label{sec:org97902c3}
\item {\bfseries\sffamily TODO} Accuracy of published next visit and advanced schedule information
\label{sec:org907d96e}
\item {\bfseries\sffamily TODO} Number of filter changes
\label{sec:org1fbf7d3}
\item {\bfseries\sffamily TODO} Fraction of time scheduled in blobs
\label{sec:org21ec40c}
\item {\bfseries\sffamily TODO} Current season length maps
\label{sec:orge8f18a7}
\item {\bfseries\sffamily TODO} Maximum gap in season maps
\label{sec:org0c228b9}
\item {\bfseries\sffamily TODO} Time since last observation maps
\label{sec:org1cfd167}
\item {\bfseries\sffamily TODO} Retrospective simulation
\label{sec:org720f324}
If we simulated the time we just did, do we get what we actually got?
\end{itemize}
\section{Infrastructure needs}
\label{sec:org4b2538b}
\subsection{{\bfseries\sffamily TODO} Data collection}
\label{sec:org25835ba}
\begin{itemize}
\item Retrieving data from the EFD
\label{sec:org69332be}
\item Retrieving data from the DM butler
\label{sec:orgee794f9}
\end{itemize}
\subsection{{\bfseries\sffamily TODO} Data processing}
\label{sec:orgb26610d}
\begin{itemize}
\item Running opsim simulations
\label{sec:org1d272c8}
\item Running MAF metrics
\label{sec:orgb1b6d5a}
\end{itemize}
\subsection{{\bfseries\sffamily TODO} Data storage?}
\label{sec:orgbb6a35a}
\subsection{{\bfseries\sffamily TODO} Exploration tools}
\label{sec:org5c55241}
\subsection{{\bfseries\sffamily TODO} Dashboards}
\label{sec:org5e0d9fa}
\subsection{{\bfseries\sffamily TODO} Report generation}
\label{sec:org922b570}
\section{Available infrastructure}
\label{sec:org113a4d7}
\subsection{{\bfseries\sffamily TODO} opsim}
\label{sec:org0dabfef}
\subsection{{\bfseries\sffamily TODO} MAF}
\label{sec:org2897a29}
\subsection{{\bfseries\sffamily TODO} Engineering and Facilities Database (EFD)}
\label{sec:orgb8f9467}
\begin{itemize}
\item \href{https://ls.st/LTS-210}{LTS-210: Engineering and Facility Database Design Document}
\end{itemize}
\subsection{{\bfseries\sffamily TODO} DM Butler}
\label{sec:org3233a85}
\subsection{{\bfseries\sffamily TODO} SQuaSH/SQuaRE}
\label{sec:org79afcc3}
\begin{itemize}
\item Overview diagram
\label{sec:orgf7a6227}
\item \href{https://sqr-009.lsst.io/}{SQR-009: The SQuaSH metrics dashboard}
\label{sec:orgc2b5641}
\item \href{https://sqr-017.lsst.io}{SQR-017: Validation Metrics Framework}
\label{sec:org89e367e}
\item \href{https://sqr-019.lsst.io/}{SQR-019: LSST Verification Framework API Demonstration}
\label{sec:org5b8403f}
\item \href{https://sqr-023.lsst.io/}{SQR-023: Design of the notebook-based report system}
\label{sec:org4e954ad}
\item \href{https://sqr-026.lsst.io/}{SQR-026: Periodic report generation and publication via notebook templates}
\label{sec:orga84b076}
\begin{itemize}
\item night summaries
\label{sec:orgd5ea38b}
\item \href{https://nbreport.lsst.io/}{nbreport} for automated reports
\label{sec:orgb9bdb69}
\end{itemize}
\item \href{https://sqr-033.lsst.io/}{SQR-033: QA Strategy Working Group recommendations for SQuaSH}
\label{sec:orgf638a02}
\item \href{https://sqr-034.lsst.io/}{SQR-034: EFD Operations}
\label{sec:org2582b32}
\item nublado for interactive analysis
\label{sec:org351c734}
\item \href{https://github.com/lsst-sqre/squash}{SQuaSH github}
\label{sec:org1e55846}
\end{itemize}
\subsection{{\bfseries\sffamily TODO} faro}
\label{sec:org2391d0f}
\begin{itemize}
\item Notes here from discussions with Colin Slater of \textit{[2021-02-25 Thu]}
\label{sec:org66ae461}
\item Turns catalogs into scalar metrics
\label{sec:orgcd9f536}
\begin{itemize}
\item faro takes processed catalogs as input
\label{sec:org3f2a7e9}
\item combines and measures things in catalog space
\label{sec:orgc6afdb2}
\begin{itemize}
\item photometrec repeatability
\label{sec:org2691489}
\item astrometric repeatabaility
\label{sec:org6bde0e8}
\item other statistics
\label{sec:org399575f}
\end{itemize}
\end{itemize}
\item Operations on metric values
\label{sec:org3217314}
\begin{itemize}
\item Packages them
\label{sec:org3f7b077}
\item sends them to SQuaSH's  time series database
\label{sec:orgb861d99}
\item keeps track of how metrics change over time
\label{sec:org7c19111}
\item based on assumption of a time series of scalars, not vectors, plots or other complex data structures
\label{sec:org1e1dd61}
\end{itemize}
\item Currently used to keep track of pipeline code's progress
\label{sec:org0f93a65}
\item Generic infrastructure to convert catalogs to metrics, then store them.
\label{sec:org1e6b406}
\item Works on time series of single scalars that can be plotted as a function of time.
\label{sec:org5381a13}
\item faro feeds the time series databes that feeds SQuaSH's influx DB
\label{sec:org1baf0c7}
\item faro is a set of pipeline tasks that run in a gen3 task execution framework
\label{sec:org2dca0a6}
\item metrics themselves are stored in the butler
\label{sec:orgef94c5a}
\item much of what scheduling and survey progress would need would be in the EFD
\label{sec:orga2937da}
\begin{itemize}
\item not clear how to get data into the DM framework.
\label{sec:orgab1eb31}
\end{itemize}
\item What is the right database for these derived quantities.
\label{sec:orgb95210e}
\end{itemize}
\subsection{{\bfseries\sffamily TODO} LOVE}
\label{sec:orga8eed47}
\begin{itemize}
\item \url{https://lsst-ts.github.io/LOVE-integration-tools/html/index.html}
\label{sec:org3c11f91}
\item \href{https://lsst-ts.github.io/LOVE-integration-tools/html/modules/overview.html\#love-architecture}{Architecture diagram} (section 1.2)
\label{sec:orga667d4e}
\item Communication consists of (from \url{https://lsst-ts.github.io/LOVE-integration-tools/html/modules/overview.html})
\label{sec:orgb597787}
\begin{itemize}
\item telemetry
\label{sec:org74cf2b5}
\item events (including "observing log events")
\label{sec:orge4f559a}
\item commands
\label{sec:org506d4df}
\item command acknoweldgement
\label{sec:orgedc518a}
\end{itemize}
\item Telemetry collected of great interest to strategy and progress tracking, but usually too low level
\label{sec:org0d1f9c8}
\end{itemize}
\section{{\bfseries\sffamily MAYBE} Possible new work needed}
\label{sec:orgbbc9e6d}
\subsection{New plots and metrics}
\label{sec:org89c6b05}
\begin{itemize}
\item Lower level than science collaboration metrics, higher level than telemetry
\label{sec:org732b728}
\end{itemize}
\subsection{Infrastructure to run and analyze new simulations}
\label{sec:org88d6b1f}
\begin{itemize}
\item Automated execution of opsim and MAF metric calculation
\label{sec:org63adc37}
\item From tonight to the end of survey
\label{sec:org09ff629}
\item For tonight under a variety of conditions
\label{sec:org0631904}
\end{itemize}
\subsection{Progress dashboard}
\label{sec:orgd411e78}
\begin{itemize}
\item Reference data (opsim inputs)
\label{sec:orgf64686d}
\begin{itemize}
\item seeing model vs. achieved
\label{sec:org59af002}
\item cloud model vs. achieved
\label{sec:orgadb76a4}
\item slew time
\label{sec:org19160a9}
\item filter change time
\label{sec:org9bee779}
\end{itemize}
\item Achieved progress
\label{sec:org43da1d9}
\begin{itemize}
\item Plots and metrics generated by MAF
\label{sec:org4036ef5}
\item Currently achieved vs. expected metric values
\label{sec:org5f87a74}
\item Metric values extrapolated to end of survey
\label{sec:orgc1eaad8}
\end{itemize}
\item Maybe two? (LOVE and SQuaRE)
\label{sec:org5fca5bb}
\begin{itemize}
\item Not all metrics may be best calculated by the same infrastructure
\label{sec:orgb7ac0ff}
\end{itemize}
\end{itemize}
\subsection{Report creating infrastructure}
\label{sec:org27d46e9}
\begin{itemize}
\item Night plans
\label{sec:orgc9d8422}
\item Night reports
\label{sec:org19ce4fd}
\item Weekly and/or monthly reports
\label{sec:orgcef4f28}
\end{itemize}
