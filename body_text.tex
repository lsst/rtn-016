
\section{Summary}
\label{sec:orgdea9fb9}
\begin{itemize}
\item Tracking survey progress, comparison of survey progress to a baseline, and validation of scheduler assumptions are important for Rubin Observatory staff and management, the science collaborations, the astronomical community, funding agencies, and even the general public.
\item Survey progress and strategy diagnostics will need to be presented in multiple contexts, to a variety of audiences. Generation and presentation of such diagnostics will require multiple tools and processes.
\item Data useful for evaluating survey progress and optomizing strategy is best visualized in a variety of ways. Some useful metrics are best viewed as scalars, or a simple time series. In other cases, complex plots or diagrams are required, and even then such plots may only be effectively interpreted in combination with other plots, diagrams, or descriptive text.
\item Some metrics and visualizations needed for evaluating survey progress and optomizing strategy are "close to the data," and require minimal processing before presentation to the user. In other cases, significant analysis is necessary (for example suites of scheduler simulations). This diversity complicates infrastructure requirements.
\item Generation of survey progress and strategy diagnostics requires both data generated "close to the instrument" (telemetry from the telescope) and data only available from other sources such as the output of processing by Data Management and the results of strategy simulations.
\item Rubin Observatory infrastructure currently under development (including opsim, MAF, SQuaRE/SQuaSH, LOVE, the Engineering and Facilities Database (EFD), and the Data Management butler) provides or will provide an array data sources and visualzation tools. It is not always clear, however, which tools are a good match for which strategy and progress related metrics, and work is required take advantage of this existing infrastructure.
\end{itemize}
\section{{\bfseries\sffamily ACTIVE} Introduction}
\label{sec:orgfd16f68}

Progress and strategy diagnostics (metrics, plots, and other figures) are usefully characterized by several features.

\begin{description}
\item[{Audience}] Different diagnostics and figures are useful to different audiences. Possible audiences include reviewers for and administrators of funding agencies, Rubin Observatory management, LSST science collaboration scientists, the Rubin Observatory observing scientists, the scheduler developers, astronomers working on other projects, other members of the astronomical community, and the general public.
\item[{Format}] The diagnostics may take any of serveral forms, including simple scalars, time series and other plots, maps, or other represntations of distributions.
\item[{Timing}] Diagnostics will need to be produced on range of timescale, ranging from hours to months. Some need to be produced shortly before or after each night of observing, or even periodically throughout the night. Others can be produced on a monthly schedule, or only in preparation for meetings and reviews.
\item[{Required data}] Data required to produce different diagnostics may originate in a variety of sources, including observatory telemetry, fully analyzed data products from Data Management, or sources outside the project (e.g. weather services).
\item[{Computing resources}] In some cases diagnostics can be produced with minimal calculation from available data sources. In other cases significant processing, up to and including suites of \texttt{opsim} simulations and corresponding calculation of metrics using \texttt{MAF}, will be required.
\end{description}

Each of these characteristics place requirements on the tools used to generate and provide access to the diagnostics.

\section{Metrics, plots, and other diagnostics}
\label{sec:orgc89e6cf}
\subsection{Survey state descriptions}
\label{sec:org5dd4ea0}
Figures and diagrams describing the current "state of the survey" will be of great interest to the community as a whole, and and providing an understanding of whether or not they correspond to the plan for the survey will be essential for explaining how well the survey is proceeding.
Examples of such diagrams will include:

\begin{description}
\item[{depth maps}] Maps of the survey showing the numbers of total visit, numbers of visits in each filter, and coadd limiting magnitude in each filter are likely to be the most prominant figures that show the current state of the survey. Maps cut in an assortment of ways, for example including only visits with PSF widths narrower than some cutoff, will also be of significant interest.
\item[{time use hourglass plots}] A single graphic that summarizes how time has been used by the survey so far will also be of wide interest. A plot that color-codes the use of time night on one axis and time of night on the other is an effective means of conveying this information. Likely categorize to by symbolized by colors include: down time due to equipment failure, down time due to regular maintenance or engineering, down time due to weather, observing time without useful exposures (e.g. due to overcast skies), time spent on observations outside the WFD footprint (possible separated by mini-survey), time spent on DDFs, and other time in the WFD.
\item[{data quality hourglass plots}] A plot similar to the time use hourglass plot, but instead of color coding separate science programs, represent data quality (\(5\sigma\) limiting magnitude relative to nominal depth).
\item[{transient detection efficiency maps}] Maps of detection efficiency for different classes of variable objects (bright and faint near Earth objects (NEOs), TransNeptunian Objects (TNOs), Tidal Disruption Events (TDEs), fast microlensing events, type 1a supernovae, and others) over the footprint, for the entire survey so far and for the current season.\footnote{Gaps between seasons may be defined to be gaps between visits than include the date on which the sun has the same R.A. as the point on the sky}.
\item[{deep drilling field (DDF) cadence and transient efficiency plots}] A set of plots (one for each DDF) showing the nights on which each DDF has been observed in the current season, and marking the time since it was last observed in each filter. Such plots may also encode the depth of exposures in each band on each night. The detection efficiency and actual numbers of detections for different classes of transients may be plotted on the same time axis, showing how gaps of different sizes affect the science for different classes of objects.
\end{description}

\subsection{Time series progression of scalar survey metrics}
\label{sec:orgd3c7acd}
Many science metrics are expected to improve continuously over the course of the survey.
For each metric, there are two and perhaps three quantities that can be usefully compared:
\begin{description}
\item[{baseline}] the value of the metric for the given time, as measured from a reference simulation.
\item[{estimated}] the value of the metric for the given time, measured from the actually collected visits and visit parameters in the same way they were measured against the baseline simulation.
\item[{achieved}] the value of the metric as mearused from the final processed data products.
\end{description}

Estimated and achieved may differ in cases where the metric ultimately depends on the final catalogues of objects, which can only be estimated using the simple list of visits and data quality parameters produced by \texttt{opsim}.
One example of this would be the total number of stars and galaxies detected (PSTN-51 sections 3.3 and 3.6): errors and limited precision in the model for the distribution of stars and galaxies will result in a difference between the estimated and achieved values of the metric.

\begin{description}
\item[{Total, mean, median, min, max, and quantiles of numbers of visits}] Table 23 of \href{http://ls.st/lpm-17}{LPM-17}, "The LSST System Science Requirements Document," gives specifications for the "sum of the median number of visits in each band, Nv1, across the sky area". Additional statistics beyond the median are also indicative of the quality of the survey: highly skewed distributions, long tails to the distribution, or a significant difference between the mean and median could all indicate problems in scheduling. Good candidates for showing the time series of this distributions would be a time series boxplot or violin plot.
\item[{Numbers of science visits by band}] In addition to the sum across all bands, the distributions of visits in each band individually, and relative to each other, are also good indicators of whether the scheduler is behaving as expected. Time series plots of the visits split by band should rougly a constant proportionality on a timescale of months, but differences within each lunation, due to filters being swapped out and redder filters being preferentially chosen when the moon is very bright.
\item[{Numbers of science visits by R.A.}] Because the visibility of areas of the sky varies with the time of year, the distribution of visits across the sky is not expected to be uniform. Ideal visibility varies with R.A., so if the survey is ultimately to be uniform, the calendar observing dates elapsed and remaining need to corresponding rougly to the distribution of completed and needed visits for uniformity across the footprint. Plots of the number of visits in a set of R.A. bins as a function of date should show clear jumps at times when those R.A.'s correspond to local Sidereal times (LSTs) during the night in those times of year.
\item[{Numbers of science visits by program}] The fraction of time dedicatated to the Wide-Fast-Deep (WFD) survey and mini-surveys (including the Deep Drilling Fields (DDFs), Galactic Plane (GP), North Ecliptic Spur (NES), South Celestial Pole (SEP), and Target of Opportunity observations (ToOs)) will be specified as part the establishment of survey strategy, and whether the scheduler is adhering to these decisions should be monitored.
\item[{Area with multiple observations separated by nearly uniformly sampled timescales}] Section 3.4.0.2 of \href{http://ls.st/lpm-17}{LPM-17} specifies a minimum area with fast (from 30 sec to 40 min) revisits.
\item[{Science collaboration metrics}] Section \href{https://ls.st/pstn-051}{PSTN-51}, "Survey Strategy and Cadence Choices for the Vera C. Rubin Observatory Legacy Survey of Space and Time (LSST)," specifies a set of metrics contributed by the LSST science collaborations: completenes for various types of solar system objects (NEO-faint, NEO-bright, TNOs), numbers of stars and galaxies detected (N stars, N gals), number of tidal disruption events (TDEs), Fast Micro Lens event, the dark energy 3x2 Figure of Merit (FoM), a supernova Ia lightcurve metric, and a Proper Motion metric. (The Weak Lensing (WL) systematics metric and large scale structure (LSS) metric are redundant with the mean number of visits and number of detected galaxes metrics, already listed.) Supplements and refinements to these metrics are expected.
\end{description}

\subsection{Predicted scalar metrics for the final survey as a function of time}
\label{sec:org0a99696}
Many metrics for the ultimate state of the survey do not improve smoothly with time.
For example, the accumulated visits will be spread roughly uniformly over the survey footprint at any given time, so the area of sky observed to the nominal depth (specified in table 22 of \href{http://ls.st/lpm-17}{LPM-17}) will remain near zero for most of the survey, and then rise rapidly at the end: simply tracking the area covered to the nominal depth as a function of time does not provide a useful indication of progress being made toward achieving this metric.
Progress toward achieving this requirement can, however, be tracked using simulations: if the remainder of the survey is simulated after each night of observing and the final metrics measured using the final result, the time series can be plotted to indicate how much progress the survey is making in comparison with what is required.
A flat horizontal line will indicate a survey progressing exactly as expected based on simulations.
A rising line will indicate that the survey is making more progress than expected, while a falling one indicates that the survey is falling behind.

The final area is not the only metric for which these simulations are good tools for indicating progress; all of the scalar science metrics provided by the science working groups can be plotted in the same way. Plotting a selection of such metrics can show whether the current conditions or startegy are favoring some science goals over others in unexpected ways.
The list of metrics tracked this way should be the same as that in section \ref{sec:orgd3c7acd}.

\subsection{Nightly scheduler behaviour diagnostics}
\label{sec:orged267d1}
A number of plots and metrics will be needed to provide useful diagnostic information that can either help explain or predict the scheduler's behaviour, or identify potential problems in it.

These metrics can be usefully tracked at two times:
\begin{description}
\item[{start of night}] At least one \texttt{opsim} simulation should be run for the night before each night of observing. Nightly scheduler behaviour diagnostics should be calculated for these simulations, giving and indication of what is expected for the night, and providing advanced warning for any unexpected or anamolous behaviour. Nights of observing do not always proceed according to plan: slight differences in the start time or overhead between exposures may cause the predicted and actual schedule to diverge, and closures due to poor weather or equipment failure may create greater disruption. A handful of simulations with random offsets in start times and overheads between exposures can indicate the range of possibilities.\footnote{If the scheduler is modified to respond to observing conditions, then a handful of weather conditions will need to be simulated as well.}
\item[{end of night}] Nightly scheduler diagnostics should be calculated for each night shortly after the completion of observing. These diagnostics will alert the project to any scheduler problems or misbehaviour during the night, and help explain the scheduler's behaviour when it was not intuitive.
\end{description}

Examples of such diagnostics include:

\begin{description}
\item[{Feature maps}] Modern version of \texttt{opsim} select visits (or sets of visits grouped into "blobs") based on "features" that are functions of location on the sky: the slew time to reach the location on the sky, the expected depth of exposures to be taken there, and the progress made so far on that portion of the sky. A weighted mean of these features determines the selection of the next visit or set of visits. Examination of maps of these visits and the resultant final reward function is therefore fundamental to understanding the scheduler's behaviour. The presentation of the feature maps is complicated by the variablity with time and dependence on current pointing. Two candidate formats may be useful:
\begin{description}
\item[{animation}] a movie of the map over the night will provide the most detail, but can be difficult to navigate.
\item[{maximum feature values}] maps of the maximum value each point on the sky takes over the course of the night will be a useful indicator of how likely the scheduler is to schedule visits in these areas at some point in the night.
\end{description}
\item[{Full table of scheduled visits}] To support coordination between Rubin Observatory and other projects, \href{https://ls.st/lse-61}{LST-61}/DMS-REQ-0353 requires that "A service shall be provided to publish to the community the next visit location and the predicted visit schedule provided by the OCS. This service shall consist of both a web page for human inspection and a web API to allow automated tools to respond promptly." Such a table (in both forms) will be useful not only to external projects, but also to the observing scientists for understanding what to expect from the upcomming night, and for the scheduling team to catch potential issues with the scheduler.
\item[{Pointing movie}] A movie of the pointings of the telescope over the course of the night will be one of the fastest ways to convey an understanding of what the scheduler will do (before the night) or did (after it). Superposition of the pointings over the feature map.
\item[{Global observing efficiency}] The ratio of the total science exposure time to the available time (measured using morning and evening twilight as references) provides a good, gross indicator of whether the scheduler is scheduling visits efficiently (minimizing overhead time).
\item[{Gap distribution}] A histogram of the gaps in time between successive visits can indicate where inefficiencies in observing come from.
\item[{Table of long gaps}] Long gaps between exposures indicated either problems or inefficiencies. A short table of unusually long gaps between pairs of exposure with possible indicators of explanations (e.g. the slew angle between exposures, or whether there was a filter change) can call attention to this lost time for evaluation by a human.
\item[{H.A. distribution}] The distribution of hour angles for scheduled exposures indicates whether the scheduler is maximizing data quality.
\item[{DDF cadence plots}] The DDF cadence plots described in section \ref{sec:org5dd4ea0} will also be important for understanding whether a DDF should be (or should have been) observed on a given night.
\item[{Fraction of time in blobs}] The "blob" scheduler is intended to be the workhorse scheduler for the WFD, and if an unexpectedly large number of exposures are being scheduled by the geedy scheduler, it may indicate a problem.
\end{description}

\subsection{Validation of the site and telescope model}
\label{sec:orgd296f78}
\texttt{opsim} simulations rely on several models for the characteristics of the site and the performance of the instrument.
Deviations from the models can have significant consequences for the accuracy of the simulations.
Comparisons between the modeled and achieved characteristics of the site and instrument will be important not only for understanding deviations between simulated and achieved performance, but also for improving simulations and making corresponding refinements to survey strategy.

For each modeled feature, there are at least two plots are of interest: one plots the measured values against the value calculated by the \texttt{opsim} model; and the other that tracks the distribution of residuals over time (for example a box or violin plot).
In some cases, additional plots may also be important.

Examples of modeled characteristics include:
\begin{description}
\item[{slew time}] In addition to simple compaing the modeled to achieved slew time, residuals between the two can be shown as a function of horizon coordinates and rotator angle.
\item[{filter change time}] Nominally 90 seconds plus up to 30 seconds to put the camera into the necessary position.
\item[{shutter time}] Nominally 1 second of overhead per visit.
\item[{readout time}] Nominally 2 seconds, in parallel with any slew time.
\item[{total overhead between successive exposures}] In principle the total overhead can be calculated by combining each source of overhead, but measurements of the total time from one exposure to the next (the start of of one visit to the start time of the next) will be an important diagnositic for discovering if the different values combine as expected, and if there are additional sources of overhead that have not been accounted for.
\item[{sky brightness}] The sky brightness as a function of airmass, sun and moon location and phase, filter, and other factors. Plots that show residuals with respect to horizon coordinates may also be useful for indicating limitations in the model due to light polution, which is not currently included in the model.
\item[{atmospheric seeing}] \texttt{opsim}'s simulation is based on achived data from the Gemini South DIMM. A comparison of the Gemini South and Rubin Observatory DIMM measurements will provide a diagnostic for resultanting limitations.
\item[{final delivered PSF width}] The final delivered PSF width is a function of the atmospheric seeing, filter, airmass, the turbulance outer scale, dome seeing, and other instrumental contributions. In some cases, the value used by the \texttt{opsim} model is highly uncertain (e.g. the turbulence outer scale). Other contributers (for example the effect of the strength and direction relative to the telescope pointing on the dome seeing) are not currently modeled at all.
\item[{extinction and lost time due to clouds}] The modules used for strategy simulation by \texttt{opsim} are based on historical cloud data recorded by humans at the nearby Cerro Tololo Inter-American Observatory. The correspondence between these estimates and actual time lost is highly uncertain.
\item[{time lost due to engineering activities and equipment failures}] 

\item[{achieved depth}] The expected \(5 \sigma\) limiting magnitude for point source detections in each visit is one of the basic "features" used by the scheduler, and is affected by a variety of factors. Comparisons between estimated and achieved depth are therefore of fundamental importance.
\end{description}

While some of these characteristics are functions of others, independent measurement of each will be important for verifying that the relations are those that are expected, and that there are no significant unaccounted for contributions.

In many cases, these characteristics will be tracked as part of telescope operations, independent of direct strategy considerations.
However, tracking and maintaining survey strategy requires presentation in a way that supports easy comparison to and updating of the \texttt{opsim} models.
Either the tracking and monitoring being done for other systems should include the necessary comparisons to the \texttt{opsim} models, or separate variations should be generated for the strategy team.

\subsection{Disruption consequence analysis}
\label{sec:orge003643}
The project will need to be able to quantify the consequences of departures from the final baseline strategy, both in advance and in retrospect. 
Possible causes of disruptions include "target of opportunity" observing and unexpected engineering downtime. 
It both cases, the consequences will not always be immediately obvious.
For example, a set of target of opportunity exposures will not necessarily result in complete loss of time for the WFD or other programs, because exposures scheduled for the ToO will often contribute to the FWD themselves. 
To quantify the effects of such disruptions, achieved metrics need to be compared to what they would have been without the disruptions.
This comparison requires additional simulations.
By comparing metrics derived from simulations in which the discruption never takes place with ones in which they did, both the immediate and long term effects of the disruption can be quantified.
The details of what simulations are needed for the comparison depend on whether the disruption being analyzed is one that has already occurred, or one which is under consideration or expected.
When evaluating possible future disruptions, the simulations for comparison are both simulations from the current time to the completion of the survey, differing only by whether or not the disruption occurs.
When evaluating the effects of a past disruption, the reference simulation (the one without the disruption) must begin in the past, before the disruption, and be run with the same environmental parameters (e.g. clouds and seeing) as actually achieved.
That way, the consequences of the disruption itself can be evaluated independentely of deviations between the simulated and actual survey.

In both cases, short and long term differences are of interest.
Two disruptions may have similar short-term effects on metrics, but some disruptions will be easier for the automatic scheduler to automatically recover from with future observations than others.
The time and degree to which it will be possible to recover from the disruption will sometimes be important information.

\section{Tools, reports, and their users}
\label{sec:org8cb5a99}
\subsection{Introduction}
\label{sec:org9679a1c}
The Rubin Observatory project and staff performing LSST will require survey progress and status diagnostics, including a variety of metrics and plots.
Some of these will be needed by the staff themselves, providing the data needed to prevent and diagnose problems, identify potential imporvements, and evaluate suggestions for changes.
In addition, such plots and metrics will also be needed for reports to the astronomical community and funding agencies, and even may be useful in engaging the general public.

The infrastructure suitable for producing such plots and metrics depend of several factors, including the audience expected to make use of them and the frequency with which they need to be produced.
Full automation of the production of plots and metrics will be most important when they need to be produced frequently, on a nightly or monthly basis.
When their audience includes non-experts, either full automation or simple production on demand will save effort.
Plots that are used primarily for debugging or exploration of specific issues may not require the same level of automation or simplicty of interface, but tools for reproduction of previous example of such diagnostics can be important for avoiding duplication of effort.

These plots and metrics can be produced and presented in any of several ways:

\begin{description}
\item[{Interactive tools}] When developing and debugging the software, hardware, and human procedures that produce the survey, experts working on the project require flexible tools to obtain and explore the relevant data. Planning and prediction of the consequences of events and choices will often benefit creation of simulations. Examples of such sets of tools (e.g. \texttt{opsim} and \texttt{MAF}) have been developed as part of project construction, and will continue to perform an important role in operations. General tools designed for monitoring other aspects of the survey (e.g. the health of the instrument or the status of data processing) will also have important roles to play.
\item[{Information dashboards}] Some plots and metrics will routine production and monitoring, often by those who are not expert users of the interactive tools like \texttt{MAF}. Even for those who do have the expertise, automation of the production of routine plots and metrics will save significant effort. Infrastructure that generates needed plots and metrics and presents them in a simple way (e.g. an automatically update web page, or small collection of web pages) will therefore be important. This infrastructure will require many of the same software components used by the interactive tools, plus some automation and presetation elements.
\item[{Reports}] The project will need to produce reports covering survey status and progress, whether in the form of documents and presentations. Many of the plots and metrics displayed in an information dashboard will be important elements in these reports.
\end{description}

The intended audience and the frequency of reporting are both important feature to consider in determining how any given metric or plot is to be generated.
Possible audiences include the funding agencies, Rubin Observatory management, LSST science collaboration scientists, the Observatory Science team (including the Observatory Scientist), the Observing Specialists, the Observatory Support Scientists, the Scheduler Scientists, astronomers working on other projects, other members of the astronomical community, and the general public. 
Plots and metrics may be generated on regular schedules (nightly, monthly, or quarterly), or as occasions demand.

The LSST system and data management requirements (\href{https://ls.st/lse-29}{LSE-29} and \href{https://ls.st/lse-61}{LSE-61}) and observatory systems specifications (\href{https://ls.st/lse-30}{LSE-30}) include requirements on several reports and reporting tools. The roles and activities in the \href{https://docushare.lsst.org/docushare/dsweb/Get/Document-36797/Rubin\%20Observatory\%20Operations\%20Plan\%20April\%202020.pdf}{Rubin Observatory Operations Plan} imply additional reports, and imply additional requirements on those already described.

\subsection{Night Plan}
\label{sec:org87d80b7}

Potential problems related to strategy or scheduling should be found and resolved before each night of observing, of possible, and the observing specialists on shift during the night need to be briefed and provided with a written plan describing any unusual activities or modes of operation, what they should expect of the scheduler, what behaviour they should consider anomalous, and how they should react to anomalous behaviour.
The \href{https://docushare.lsst.org/docushare/dsweb/Get/Document-36797/Rubin\%20Observatory\%20Operations\%20Plan\%20April\%202020.pdf}{Rubin Observatory Operations Plan} gives responsibility for reviewing and supervising scheduler behaviour to the Observatory Scientist and the Observatory Support Scientists, but specific procedures for this review and the briefing of the observing specialists are not yet developed.
Under any plan it would be worthwhile to automate the generation of the necessary scheduler simulations and diagnostics (listed in section \ref{sec:orged267d1}) for review and inclusion in briefing for the observing specialists and a plan for the night.

In operations rehearsals (summarized in \href{https://dmtn-119.lsst.io}{DMTN-119} and \href{https://dmtn-159.lsst.io/}{DMTN-159}), each night was planned in a daily meeting which included the current status and plans for the next night.
Among the "lessons learned" described in DMTN-119 was the need for a good note-taking during the daily meeting, with status report elements filled in prior to the meeting itself. 
The minutes of this meeting can then become a plan for the night.
Infrastructure to automate the creation of these report elements could either present them using a dashboard-like interface and be incorporated into the minutes, or create a template night plan directly, then supplemented during the meeting.\footnote{This process is similar to that used for observing for the Dark Energy Survey (DES).}

Automation in support of the night plan should include:
\begin{itemize}
\item Automatic creation of one or more scheduler simulations.\footnote{A side-effect of the creation of scheduler simulations completed in the afternoon is the creation of one or more candidate schedules. If these are produced in format that can be uploaded to the OCS, they can serve as a back-up to the scheduler in the unlikely event of a catastrophic failure of the scheduler during the night.}
\item Automatic creation of the diagnostics listed in section \ref{sec:orged267d1}, based on the simulations.
\item Presentation of the diagnostics a dashboard, automatically generated static report, or as part of a template observing plan for the night.
\end{itemize}

\subsection{Published upcomming schedule}
\label{sec:orgcaaed7c}
To support coordination between LSST observing and that of other projects, including schedling of simultaneous or nearly simultaneous exposures the same areas of sky, the requirments specify that Rubin Observatory publish the observing schedule in advance.

One requirement that specifies the advanced schedule is \href{https://ls.st/lse-61}{LST-61}/DMS-REQ-0353, "Publishing predicted visit schedule":
\begin{quote}
Specification: A service shall be provided to publish to the community the next visit location and the predicted visit schedule provided by the OCS. This service shall consist of both a web page for human inspection and a web API to allow automated tools to respond promptly.

Discussion: The next visit and advanced schedule do not need to be published using the same service or protocol.
\end{quote}
another is \href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0378, "Advanced Publishing of Scheduler Sequence":
\begin{quote}
The scheduling of the observing sequence lasting at least \texttt{schedSeqDuration} shall be published in advance of each observing visit.
\end{quote}

These requirments imply the infrastructure necessary for:
\begin{itemize}
\item Automatic creation of scheduler simulations. The initial simulation for the night may be the same one as that described in section \ref{sec:org87d80b7}, but additional simulations throughout the night will also be required.
\item A service to publish the predicted schedule through a web API.
\item A service te publish the predicted schedule on a web page suitable for human inspection.
\end{itemize}

The overlap between these requiremnts and those for the creation of a night plan suggests that the same tool be used for both uses. 
Support for this use case imposes several additional requirements not present for the night plan:
\begin{itemize}
\item The published schedule and diagnostics must be available to the public, not just the project staff.
\item Update schedules need to be published as necessary though the night, not just at the start of the each night.
\item A web API suitable for support of automated tools must be supplied.
\end{itemize}

\subsection{Night reports}
\label{sec:org59e9f15}
Night reports (or nights summaries) are in important feature common to most astronomical facilities, and basic plots and metrics indicating survey progress are important elements for such reports in large surveys such as LSST.
Several Rubin Observatory requirements require specify different aspects of the content and creation of night reports, including LSE-30/OSS-REQ-0131, LSE-30/OSS-REQ-0406, LSE-61/DMS-REQ-0096, and LSE-61/DMS-REQ-0097. Section 1.4 of LST-490, the "Observatory Electronics Logging Working Group Report," acknowledges the need for infrastructure to support the creation of this report.
The specifications require that the report summarize "per system performance and behavior," but do not specify what is to be reported in great detail.
This report is a natural home for the nightly scheduler behaviour diagnostics (described in section \ref{sec:orged267d1}), when applied to actual (as opposed to simulated) scheduled nights.
Furthermore, some elements of the survey state description (section \ref{sec:org5dd4ea0}) will be of broad enough interest that updates to them may be usefully included after each night.

In addition to the diagnostics directly related to scheduling, several of the data quality indicators that will be reported in the night report (following LSE-61/DMS-REQ-0097) to monitor the health of other subsystems are close to those needed for validation of the scheduler's site and telescope model (section \ref{sec:orgd296f78}). If these elements are produced with the needs of the scheduler scientists in mind, these same plots may fill both needs.

So, to support scheduler and survey progress monitoring, the night report should include:
\begin{itemize}
\item Comparisons of system characteristics (slew time, filter change time, depth, sky brightness, etc.) with models used by the scheduler simulator.
\item Nightly scheduler behaviour diagnostics
\item 
\end{itemize}

\subsection{{\bfseries\sffamily ACTIVE} Tools for performance evaluation and analysis}
\label{sec:orgf9c7497}
The Observatory Scientist, Observatory Support Scientists, and the Survey Scheduling team will need to routinely monitor survey progress and assumptions at a more detailed level than supported by the night log alone.
This monitoring will require routine monitoring of most or all diagnostics listed in section \ref{sec:orgc89e6cf}. 
Such routine monitoring if the full set of these plots were generated automatically after every night of observing. 

\subsection{Periodic progress reports and performance reviews}
\label{sec:orgd815393}
The observatory staff and scheduling team will need to report progress and strategic concerns to management, funding agencies, and the community as a whole. Requirements for the existence of such reports are present in multiple plans and requirements documents. Some examples include \href{https://ls.st/lse-29}{LSE-29}/LSR-REQ-0065, "Survey performance reviews," which states:
\begin{quote}
The Observatory shall have the ability to provide periodic status reports on the progress of the survey to allow both operations staff and the community to assess the survey progress.
\end{quote}
and \href{https://ls.st/lse-30}{LSE-30}/OSS-REQ-0033, "Survey Planning and performance monitoring", calls out the need for reporting to the community at large:
\begin{quote}
The LSST shall provide the tools and administrative processes necessary to monitor the progress of the ongoing survey, provide reports on the progress of the survey, respond to feedback from the science community, and evaluate the impact of changing science priorities over the 10 year survey lifetime.

Discussion: It is expected that the performance of this task will require the use of detailed survey simulations in order to evaluate scheduling alternatives and optimize the future performance of the survey.
\end{quote}

The \href{https://docushare.lsst.org/docushare/dsweb/Get/Document-36797/Rubin\%20Observatory\%20Operations\%20Plan\%20April\%202020.pdf}{Rubin Observatory Operations Plan} gives responsibility for producing a quarterly report to the Observatory Support Scientist:
\begin{quote}
Responsible for producing a quarterly report on the scheduled/expected observations versus the performed observations. This analysis includes monitoring the assumptions used by the scheduler including slew times, shutter open/close times, readout times etc. 
\end{quote}

Multiple such reports will be made on different schedulers, customized for different audiences.
All of these reports may draw from any of the report elements descibed in section \ref{sec:orgc89e6cf}, but it is unlikely that any single report will require every element.
While the generation of individual elements will benefit from automation, the compilation and construction of such reports will require human attention and customization to each audience.

\subsection{Interfaces for education and public outreach}
\label{sec:orgf01ac2d}
While many survey progress metrics and visualizations are only likely to be of interest to experts, several will be intuitive, and may be good candidates for engaging the general public, as per \href{https://ls.st/lse-29}{LSE-29}/LSR-REQ-0113, "EPO Products, Tools, and Interfaces"
\begin{quote}
LSST EPO shall provide access to LSST data through tools, interfaces,
and learning experiences that are designedto engage communities with
different levels of knowledge, experience and skills.
\end{quote}
Good candidates for presentation to the public are movies of numbers of exposures generated, and plots numbers of galaxies (or other objects) detected as a function of time.
\section{Infrastructure needs}
\label{sec:orgbcf1e5f}
\subsection{{\bfseries\sffamily TODO} Data collection}
\label{sec:orge4cf806}
\begin{itemize}
\item Retrieving data from the EFD
\label{sec:org48efe21}
\item Retrieving data from the DM butler
\label{sec:orgfe8b992}
\end{itemize}
\subsection{{\bfseries\sffamily TODO} Data processing}
\label{sec:orgfb5a817}
\begin{itemize}
\item Running opsim simulations
\label{sec:orgc1474ca}
\item Running MAF metrics
\label{sec:org3c94abe}
\end{itemize}
\subsection{{\bfseries\sffamily TODO} Data storage?}
\label{sec:org7db853d}
\subsection{{\bfseries\sffamily TODO} Exploration tools}
\label{sec:org0952ac8}
\subsection{{\bfseries\sffamily TODO} Dashboards}
\label{sec:orgf5e3cfc}
\subsection{{\bfseries\sffamily TODO} Report generation}
\label{sec:org8e00c50}
\section{Available infrastructure}
\label{sec:org92d9ca0}
\subsection{{\bfseries\sffamily TODO} opsim}
\label{sec:org3a750ab}
\subsection{{\bfseries\sffamily TODO} MAF}
\label{sec:org4a44373}
\subsection{{\bfseries\sffamily TODO} Engineering and Facilities Database (EFD)}
\label{sec:orga72d1cd}
\begin{itemize}
\item \href{https://ls.st/LTS-210}{LTS-210: Engineering and Facility Database Design Document}
\item \href{https://sqr-034.lsst.io/}{SQR-034: EFD Operations}
\item \href{https://sqr-029.lsst.io/}{SQR-029: DM-EFD prototype implementation}
\end{itemize}
\subsection{{\bfseries\sffamily TODO} DM Butler}
\label{sec:org68e2d0f}
\subsection{{\bfseries\sffamily TODO} SQuaSH/SQuaRE}
\label{sec:org40fb623}
\begin{itemize}
\item Overview diagram
\label{sec:org7324a97}
\item \href{https://sqr-009.lsst.io/}{SQR-009: The SQuaSH metrics dashboard}
\label{sec:org4393ea4}
\item \href{https://sqr-017.lsst.io}{SQR-017: Validation Metrics Framework}
\label{sec:org26f3697}
\item \href{https://sqr-019.lsst.io/}{SQR-019: LSST Verification Framework API Demonstration}
\label{sec:org75ef344}
\item \href{https://sqr-023.lsst.io/}{SQR-023: Design of the notebook-based report system}
\label{sec:orgd410917}
\item \href{https://sqr-026.lsst.io/}{SQR-026: Periodic report generation and publication via notebook templates}
\label{sec:org263ab57}
\begin{itemize}
\item night summaries
\label{sec:org769e951}
\item \href{https://nbreport.lsst.io/}{nbreport} for automated reports
\label{sec:orgfdc5066}
\end{itemize}
\item \href{https://sqr-033.lsst.io/}{SQR-033: QA Strategy Working Group recommendations for SQuaSH}
\label{sec:orgde1e535}
\item \href{https://sqr-034.lsst.io/}{SQR-034: EFD Operations}
\label{sec:orgf6f6505}
\item nublado for interactive analysis
\label{sec:org08524cd}
\item \href{https://github.com/lsst-sqre/squash}{SQuaSH github}
\label{sec:orge729cff}
\end{itemize}
\subsection{{\bfseries\sffamily TODO} faro}
\label{sec:org7cf33ac}
\begin{itemize}
\item Notes here from discussions with Colin Slater of \textit{[2021-02-25 Thu]}
\label{sec:org14aaceb}
\item Turns catalogs into scalar metrics
\label{sec:org8a24420}
\begin{itemize}
\item faro takes processed catalogs as input
\label{sec:orgf9bb7e8}
\item combines and measures things in catalog space
\label{sec:org557a537}
\begin{itemize}
\item photometrec repeatability
\label{sec:org93c8071}
\item astrometric repeatabaility
\label{sec:org8b80d4f}
\item other statistics
\label{sec:orgff76809}
\end{itemize}
\end{itemize}
\item Operations on metric values
\label{sec:org18f8189}
\begin{itemize}
\item Packages them
\label{sec:org2828110}
\item sends them to SQuaSH's  time series database
\label{sec:orgf05c520}
\item keeps track of how metrics change over time
\label{sec:org9a99b57}
\item based on assumption of a time series of scalars, not vectors, plots or other complex data structures
\label{sec:orge7908f4}
\end{itemize}
\item Currently used to keep track of pipeline code's progress
\label{sec:orgcb951c3}
\item Generic infrastructure to convert catalogs to metrics, then store them.
\label{sec:org6d5dca2}
\item Works on time series of single scalars that can be plotted as a function of time.
\label{sec:org26dae85}
\item faro feeds the time series databes that feeds SQuaSH's influx DB
\label{sec:org4fa9825}
\item faro is a set of pipeline tasks that run in a gen3 task execution framework
\label{sec:org0e271db}
\item metrics themselves are stored in the butler
\label{sec:org513dafb}
\item much of what scheduling and survey progress would need would be in the EFD
\label{sec:orgaa24f6f}
\begin{itemize}
\item not clear how to get data into the DM framework.
\label{sec:orge7e522d}
\end{itemize}
\item What is the right database for these derived quantities.
\label{sec:org17db8cb}
\end{itemize}
\subsection{{\bfseries\sffamily TODO} LOVE}
\label{sec:org4954386}
\begin{itemize}
\item \url{https://confluence.lsstcorp.org/pages/viewpage.action?pageId=60950797}
\label{sec:org80484b6}
\item \url{https://lsst-ts.github.io/LOVE-integration-tools/html/index.html}
\label{sec:org3507869}
\item \href{https://lsst-ts.github.io/LOVE-integration-tools/html/modules/overview.html\#love-architecture}{Architecture diagram} (section 1.2)
\label{sec:orgdac236b}
\item Communication consists of (from \url{https://lsst-ts.github.io/LOVE-integration-tools/html/modules/overview.html})
\label{sec:orgc223ad3}
\begin{itemize}
\item telemetry
\label{sec:org0aa638d}
\item events (including "observing log events")
\label{sec:org6c20b46}
\item commands
\label{sec:org5e73f09}
\item command acknoweldgement
\label{sec:org1108e3a}
\end{itemize}
\item Telemetry collected of great interest to strategy and progress tracking, but usually too low level
\label{sec:orgc9b3a7a}
\end{itemize}
\subsection{{\bfseries\sffamily TODO} Observatory Logging Ecosystem}
\label{sec:org6f99a5f}
\begin{itemize}
\item \url{https://ls.st/lse-490}
\label{sec:org9cb4bae}
\item \url{https://dmtn-173.lsst.io/}
\label{sec:orgbf619be}
\end{itemize}
\section{{\bfseries\sffamily MAYBE} Possible new work needed}
\label{sec:orgb4729c9}
\subsection{New plots and metrics}
\label{sec:org407ad6c}
\begin{itemize}
\item Lower level than science collaboration metrics, higher level than telemetry
\label{sec:org6bb5f57}
\end{itemize}
\subsection{Infrastructure to run and analyze new simulations}
\label{sec:orga1a2b14}
\begin{itemize}
\item Automated execution of opsim and MAF metric calculation
\label{sec:org17b7cd9}
\item From tonight to the end of survey
\label{sec:org3ac8fca}
\item For tonight under a variety of conditions
\label{sec:orgd0b90bb}
\end{itemize}
\subsection{Progress dashboard}
\label{sec:org2c5d073}
\begin{itemize}
\item Reference data (opsim inputs)
\label{sec:org01e54a1}
\begin{itemize}
\item seeing model vs. achieved
\label{sec:org42d7adc}
\item cloud model vs. achieved
\label{sec:orgc3cda23}
\item slew time
\label{sec:org63cb3a5}
\item filter change time
\label{sec:org8764894}
\end{itemize}
\item Achieved progress
\label{sec:orgd3a6a7d}
\begin{itemize}
\item Plots and metrics generated by MAF
\label{sec:orgfe41a94}
\item Currently achieved vs. expected metric values
\label{sec:org427f5c2}
\item Metric values extrapolated to end of survey
\label{sec:orgb8a11c0}
\end{itemize}
\item Maybe two? (LOVE and SQuaRE)
\label{sec:org1361823}
\begin{itemize}
\item Not all metrics may be best calculated by the same infrastructure
\label{sec:orgb0063a1}
\end{itemize}
\end{itemize}
\subsection{Report creating infrastructure}
\label{sec:orgce1f520}
\begin{itemize}
\item Night plans
\label{sec:org43377b1}
\item Night reports
\label{sec:org270107b}
\item Weekly and/or monthly reports
\label{sec:org45e0bb6}
\end{itemize}
